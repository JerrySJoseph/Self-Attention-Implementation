# Small Model Configuration for Testing
# ~25M parameters, good for quick experiments

model:
  vocab_size: 32000
  embedding_dim: 256
  num_layers: 6
  num_heads: 4
  ff_dim: 1024
  context_length: 512
  dropout: 0.1

training:
  # Batch settings
  batch_size: 8
  gradient_accumulation_steps: 4  # effective batch = 32

  # Optimizer
  learning_rate: 5.0e-4
  weight_decay: 0.1
  max_grad_norm: 1.0

  # Schedule
  max_steps: 10000
  warmup_steps: 500
  min_lr_ratio: 0.1

  # Checkpointing & logging
  save_every: 2000
  eval_every: 500
  log_every: 50
  sample_every: 1000

  # Data paths
  train_data_path: "data/train.bin"
  val_data_path: "data/val.bin"
  tokenizer_path: "data/tokenizer.model"

  # Output directories
  checkpoint_dir: "checkpoints"
  log_dir: "logs"

  # Hardware
  mixed_precision: true
  seed: 42

  # Early stopping
  early_stopping_patience: 5
  early_stopping_min_delta: 0.001
